{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQywnYYd-zae"
      },
      "source": [
        "# End-to-End Deep Learning Framework for Automated ECG Image Diagnosis and Clinical Report Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIwmTM0B0P6C",
        "outputId": "41e90ddf-4860-4309-bfa8-7414b9135049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "✓ GradCAM imported successfully\n",
            "✓ Using device: cuda\n",
            "✓ Random seeds set for reproducibility\n",
            "\n",
            "=== Import Verification ===\n",
            "✓ PyTorch version: 2.6.0+cu124\n",
            "✓ Timm version: 1.0.15\n",
            "✓ OpenCV version: 4.11.0\n",
            "✓ NumPy version: 2.0.2\n",
            "✓ Pandas version: 2.2.2\n",
            "✓ Tesseract OCR is working\n",
            "=== Setup Complete ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup & Imports\n",
        "\n",
        "# Install required packages with correct names and dependencies\n",
        "!pip install timm scikit-learn jinja2 pandas opencv-python albumentations --quiet\n",
        "!pip install grad-cam --quiet  # Correct package name for pytorch-grad-cam\n",
        "!apt-get update && apt-get install -y tesseract-ocr --quiet  # Install Tesseract OCR\n",
        "!pip install pytesseract --quiet  # Now install Python wrapper\n",
        "\n",
        "import os, random, re, json, gc, time, threading, warnings\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torchvision import transforms\n",
        "import timm\n",
        "\n",
        "import cv2\n",
        "import pytesseract\n",
        "from jinja2 import Template\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import grad-cam with correct syntax\n",
        "try:\n",
        "    from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "    print(\"✓ GradCAM imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ GradCAM import failed: {e}\")\n",
        "    print(\"Installing grad-cam...\")\n",
        "    !pip install grad-cam\n",
        "    from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
        "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check if seaborn style exists, use alternative if not\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except OSError:\n",
        "        plt.style.use('default')\n",
        "        print(\"⚠️ Using default matplotlib style\")\n",
        "\n",
        "# Set up device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"✓ Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "print(\"✓ Random seeds set for reproducibility\")\n",
        "\n",
        "# Verify critical imports\n",
        "print(\"\\n=== Import Verification ===\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ Timm version: {timm.__version__}\")\n",
        "print(f\"✓ OpenCV version: {cv2.__version__}\")\n",
        "print(f\"✓ NumPy version: {np.__version__}\")\n",
        "print(f\"✓ Pandas version: {pd.__version__}\")\n",
        "\n",
        "# Test pytesseract\n",
        "try:\n",
        "    pytesseract.get_tesseract_version()\n",
        "    print(\"✓ Tesseract OCR is working\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Tesseract issue: {e}\")\n",
        "\n",
        "print(\"=== Setup Complete ===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om_TZtQlcTSG",
        "outputId": "92782044-fa78-4198-8176-19f54c816b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# 2. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2psL62-j0UUy",
        "outputId": "a97386e5-a698-4d80-d296-d7082414a386"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting dataset preparation...\n",
            "Processing source directory: /content/drive/My Drive/Dataset/Abnormal Heartbeat Patients\n",
            "Processing source directory: /content/drive/My Drive/Dataset/Myocardial Infarction Patients\n",
            "Processing source directory: /content/drive/My Drive/Dataset/Normal Person\n",
            "Processing source directory: /content/drive/My Drive/Dataset/Patient that have History of Myocardial Infraction\n",
            "Splitting images for class: Abnormal Heartbeat Patients (339 images)\n",
            "  Train: 237 | Val: 51 | Test: 51\n",
            "Copying and renaming images for Abnormal Heartbeat Patients...\n",
            "Splitting images for class: Myocardial Infarction Patients (358 images)\n",
            "  Train: 250 | Val: 54 | Test: 54\n",
            "Copying and renaming images for Myocardial Infarction Patients...\n",
            "Splitting images for class: Normal Person (426 images)\n",
            "  Train: 298 | Val: 64 | Test: 64\n",
            "Copying and renaming images for Normal Person...\n",
            "Splitting images for class: Patient that have History of Myocardial Infraction (258 images)\n",
            "  Train: 180 | Val: 39 | Test: 39\n",
            "Copying and renaming images for Patient that have History of Myocardial Infraction...\n",
            "\n",
            "Dataset preparation complete!\n",
            "Dataset structure created in: /content/drive/My Drive/Dataset\n",
            "Train images: 965 (approx.)\n",
            "Val images: 208 (approx.)\n",
            "Test images: 208 (approx.)\n"
          ]
        }
      ],
      "source": [
        "# 3. Preprocessing the dataset\n",
        "\n",
        "# Define base directories\n",
        "BASE_DIR = Path('/content/drive/My Drive/Dataset')\n",
        "REPORTS_DIR = BASE_DIR / 'generated_reports'\n",
        "MODELS_DIR = BASE_DIR / 'saved_models'\n",
        "TRAIN_DIR = BASE_DIR / 'train'\n",
        "VAL_DIR = BASE_DIR / 'val'\n",
        "TEST_DIR = BASE_DIR / 'test'\n",
        "\n",
        "# Create necessary directories\n",
        "REPORTS_DIR.mkdir(exist_ok=True)\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "TRAIN_DIR.mkdir(exist_ok=True)\n",
        "VAL_DIR.mkdir(exist_ok=True)\n",
        "TEST_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Define the source directories and their new prefixes\n",
        "SOURCE_DIRS_MAP = {\n",
        "    'Abnormal Heartbeat Patients': 'HB',\n",
        "    'Myocardial Infarction Patients': 'MI',\n",
        "    'Normal Person': 'Normal',\n",
        "    'Patient that have History of Myocardial Infraction': 'PMI'\n",
        "}\n",
        "\n",
        "print(\"Starting dataset preparation...\")\n",
        "\n",
        "# --- Step 1: Clean and Collect all image paths ---\n",
        "all_image_paths = []\n",
        "class_to_prefix = {} # To map original folder name to desired prefix\n",
        "\n",
        "for src_folder_name, prefix in SOURCE_DIRS_MAP.items():\n",
        "    current_src_dir = BASE_DIR / src_folder_name\n",
        "    if not current_src_dir.exists():\n",
        "        print(f\"Warning: Source directory '{current_src_dir}' does not exist. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing source directory: {current_src_dir}\")\n",
        "    class_dir_path = current_src_dir\n",
        "\n",
        "    # Get all files in the current class directory\n",
        "    for img_file in class_dir_path.iterdir():\n",
        "        if img_file.is_file():\n",
        "            # Remove \"Copy\" files\n",
        "            if \"Copy\" in img_file.name:\n",
        "                print(f\"Removing duplicate file: {img_file}\")\n",
        "                os.remove(img_file)\n",
        "                continue\n",
        "            all_image_paths.append((img_file, src_folder_name)) # Store (path, original_folder_name)\n",
        "            class_to_prefix[src_folder_name] = prefix\n",
        "\n",
        "# If no images found, exit\n",
        "if not all_image_paths:\n",
        "    print(\"No images found in the specified source directories. Exiting.\")\n",
        "else:\n",
        "    # Separate paths by class\n",
        "    images_by_class = {src_folder_name: [] for src_folder_name in SOURCE_DIRS_MAP.keys()}\n",
        "    for img_path, src_folder_name in all_image_paths:\n",
        "        images_by_class[src_folder_name].append(img_path)\n",
        "\n",
        "    # --- Step 2: Split the data into train, val, test ---\n",
        "    # Define split ratios\n",
        "    train_ratio = 0.7\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15\n",
        "\n",
        "    # Perform stratified split for each class\n",
        "    for src_folder_name, img_list in images_by_class.items():\n",
        "        if not img_list:\n",
        "            print(f\"No images found for class: {src_folder_name}. Skipping split.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Splitting images for class: {src_folder_name} ({len(img_list)} images)\")\n",
        "\n",
        "        # Create subdirectories for the current class in train, val, test\n",
        "        class_train_dir = TRAIN_DIR / src_folder_name\n",
        "        class_val_dir = VAL_DIR / src_folder_name\n",
        "        class_test_dir = TEST_DIR / src_folder_name\n",
        "\n",
        "        class_train_dir.mkdir(exist_ok=True, parents=True)\n",
        "        class_val_dir.mkdir(exist_ok=True, parents=True)\n",
        "        class_test_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Split into train and temp (val + test)\n",
        "        train_images, temp_images = train_test_split(img_list, test_size=(val_ratio + test_ratio), random_state=42)\n",
        "\n",
        "        # Split temp into val and test\n",
        "        # Calculate new test_size ratio for temp_images (test_ratio / (val_ratio + test_ratio))\n",
        "        val_images, test_images = train_test_split(temp_images, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42)\n",
        "\n",
        "        print(f\"  Train: {len(train_images)} | Val: {len(val_images)} | Test: {len(test_images)}\")\n",
        "\n",
        "        # --- Step 3: Move and Re-index/Rename files ---\n",
        "        # Function to copy and rename\n",
        "        def copy_and_rename(image_list, dest_dir, prefix_label):\n",
        "            for i, img_path in enumerate(image_list):\n",
        "                new_name = f\"{prefix_label}({i+1}){img_path.suffix}\"\n",
        "                shutil.copy(img_path, dest_dir / new_name)\n",
        "\n",
        "        prefix_label = class_to_prefix[src_folder_name]\n",
        "\n",
        "        print(f\"Copying and renaming images for {src_folder_name}...\")\n",
        "        copy_and_rename(train_images, class_train_dir, prefix_label)\n",
        "        copy_and_rename(val_images, class_val_dir, prefix_label)\n",
        "        copy_and_rename(test_images, class_test_dir, prefix_label)\n",
        "\n",
        "    print(\"\\nDataset preparation complete!\")\n",
        "    print(f\"Dataset structure created in: {BASE_DIR}\")\n",
        "    print(f\"Train images: {len(list(TRAIN_DIR.rglob('*.*')))} (approx.)\")\n",
        "    print(f\"Val images: {len(list(VAL_DIR.rglob('*.*')))} (approx.)\")\n",
        "    print(f\"Test images: {len(list(TEST_DIR.rglob('*.*')))} (approx.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkrHei-T0Wgp"
      },
      "outputs": [],
      "source": [
        "# 3. Enhanced Dataset with Advanced Augmentation\n",
        "class ECGImageDataset(Dataset):\n",
        "    label_map = {'HB': 0, 'MI': 1, 'Normal': 2, 'PMI': 3}\n",
        "\n",
        "    def __init__(self, root: Path, transform=None, is_training=False):\n",
        "        self.samples, self.transform, self.is_training = [], transform, is_training\n",
        "        self.class_counts = {i: 0 for i in self.label_map.values()}\n",
        "\n",
        "        for cls, idx in self.label_map.items():\n",
        "            pattern = rf'{cls}\\(\\d+\\)'\n",
        "            for p in root.rglob('*.jpg'):\n",
        "                if re.search(pattern, p.name):\n",
        "                    self.samples.append((p, idx))\n",
        "                    self.class_counts[idx] += 1\n",
        "\n",
        "        random.shuffle(self.samples)\n",
        "        print(f\"Dataset loaded: {len(self.samples)} samples\")\n",
        "        print(f\"Class distribution: {self.class_counts}\")\n",
        "\n",
        "    def get_class_weights(self):\n",
        "        \"\"\"Calculate class weights for balanced training\"\"\"\n",
        "        total = sum(self.class_counts.values())\n",
        "        weights = [total / (len(self.class_counts) * count) for count in self.class_counts.values()]\n",
        "        return torch.FloatTensor(weights)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        try:\n",
        "            # Load image\n",
        "            img = Image.open(path).convert('RGB')\n",
        "            img_array = np.array(img)\n",
        "\n",
        "            # Apply transforms\n",
        "            if self.transform:\n",
        "                if isinstance(self.transform, A.Compose):\n",
        "                    # Albumentations\n",
        "                    transformed = self.transform(image=img_array)\n",
        "                    img_tensor = transformed['image']\n",
        "                else:\n",
        "                    # PyTorch transforms\n",
        "                    img_tensor = self.transform(img)\n",
        "            else:\n",
        "                img_tensor = transforms.ToTensor()(img)\n",
        "\n",
        "            return img_tensor, label, str(path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {path}: {e}\")\n",
        "\n",
        "            dummy_img = torch.zeros(3, 384, 384)\n",
        "            return dummy_img, label, str(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7eDWA9_0ZoJ"
      },
      "outputs": [],
      "source": [
        "# 4. Advanced Data Augmentation Pipeline\n",
        "def get_training_transforms():\n",
        "    \"\"\"Comprehensive augmentation for ECG images\"\"\"\n",
        "    return A.Compose([\n",
        "        # Geometric transforms\n",
        "        A.Resize(384, 384),\n",
        "        A.HorizontalFlip(p=0.3),  # ECG can be flipped in some cases\n",
        "        A.Rotate(limit=3, p=0.3),  # Small rotations for scanning artifacts\n",
        "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=2, p=0.3),\n",
        "\n",
        "        # Noise and artifacts (common in ECG)\n",
        "        A.GaussNoise(var_limit=(5.0, 15.0), p=0.2),\n",
        "        A.ISONoise(color_shift=(0.01, 0.02), intensity=(0.1, 0.3), p=0.2),\n",
        "        A.MultiplicativeNoise(multiplier=(0.95, 1.05), p=0.2),\n",
        "\n",
        "        # Brightness/Contrast (for different recording conditions)\n",
        "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
        "        A.RandomGamma(gamma_limit=(90, 110), p=0.2),\n",
        "\n",
        "        # Grid artifacts (common in ECG paper)\n",
        "        A.GridDistortion(num_steps=3, distort_limit=0.05, p=0.15),\n",
        "\n",
        "        # Normalize and convert to tensor\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_validation_transforms():\n",
        "    \"\"\"Simple transforms for validation/test\"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(384, 384),\n",
        "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8qkrGXJ0dya",
        "outputId": "e27c7034-2303-4015-e38e-447281748c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded: 1145 samples\n",
            "Class distribution: {0: 237, 1: 430, 2: 298, 3: 180}\n",
            "Dataset loaded: 247 samples\n",
            "Class distribution: {0: 51, 1: 93, 2: 64, 3: 39}\n"
          ]
        }
      ],
      "source": [
        "# 5. Create datasets with enhanced transforms\n",
        "train_ds = ECGImageDataset(TRAIN_DIR, transform=get_training_transforms(), is_training=True)\n",
        "val_ds = ECGImageDataset(VAL_DIR, transform=get_validation_transforms(), is_training=False)\n",
        "test_paths = sorted(TEST_DIR.glob('*.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOB_4tLp0sDF",
        "outputId": "07ba142d-4008-43e6-f9d5-61e57c503dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: tensor([1.2078, 0.6657, 0.9606, 1.5903])\n",
            "Training batches: 36, Validation batches: 8\n"
          ]
        }
      ],
      "source": [
        "# 6. Weighted Sampling for Class Balance\n",
        "class_weights = train_ds.get_class_weights()\n",
        "sample_weights = [class_weights[label] for _, label in train_ds.samples]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "# Data loaders with optimal settings\n",
        "train_loader = DataLoader(train_ds, batch_size=32, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37lX_xJH0v-q"
      },
      "outputs": [],
      "source": [
        "# 7. Enhanced Model with Better Architecture - Using EfficientNetV2-S\n",
        "class ECGClassifier(nn.Module):\n",
        "    def __init__(self, model_name='tf_efficientnetv2_s', num_classes=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
        "        self.feature_dim = self.backbone.num_features\n",
        "\n",
        "        # Enhanced classifier head - Reduced size for smaller model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(self.feature_dim, 256),  # Reduced from 512 to 256\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout/2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.classifier:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        return self.classifier(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7NwmEMP1ApC",
        "outputId": "3a8f9afc-2c04-46ea-ac1b-f5e491fc306f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Total parameters: 20,506,452\n",
            "Trainable parameters: 20,506,452\n",
            "Mixed precision training: Enabled\n"
          ]
        }
      ],
      "source": [
        "# 8. Training Setup with Advanced Components\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = ECGClassifier(num_classes=4).to(device)\n",
        "\n",
        "# Print model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Advanced loss function with class weighting\n",
        "# Assuming class_weights is defined elsewhere - if not, we'll compute it\n",
        "try:\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device), label_smoothing=0.1)\n",
        "except NameError:\n",
        "    print(\"Warning: class_weights not found, using unweighted loss\")\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Optimizer with better settings - Slightly reduced learning rate for smaller model\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=8e-4,  # Reduced from 1e-3\n",
        "    weight_decay=0.01,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "# Advanced learning rate scheduler\n",
        "# Assuming train_loader is defined - if not, we'll use a placeholder\n",
        "try:\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=8e-4,  # Reduced from 1e-3\n",
        "        epochs=40,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "except NameError:\n",
        "    print(\"Warning: train_loader not found, using step scheduler\")\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# Mixed precision training - Fixed version\n",
        "use_amp = device.type == 'cuda'\n",
        "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
        "print(f\"Mixed precision training: {'Enabled' if use_amp else 'Disabled'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEHkG1431EZp"
      },
      "outputs": [],
      "source": [
        "# 9. Enhanced Training Functions\n",
        "def train_epoch(loader, epoch):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for batch_idx, (imgs, labels, _) in enumerate(loader):\n",
        "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        if use_amp:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update scheduler if it's OneCycleLR\n",
        "        if hasattr(scheduler, 'step_update'):\n",
        "            scheduler.step()\n",
        "        elif isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
        "            scheduler.step()\n",
        "\n",
        "        # Statistics - ensure float32 for loss computation\n",
        "        total_loss += loss.item() * imgs.size(0)\n",
        "        predicted = outputs.argmax(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        # Progress logging\n",
        "        if batch_idx % 50 == 0:\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "            print(f'Epoch {epoch} [{batch_idx}/{len(loader)}] '\n",
        "                  f'Loss: {loss.item():.4f} Acc: {100.*correct/total:.2f}% LR: {lr:.6f}')\n",
        "\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "def validate_epoch(loader):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, _ in loader:\n",
        "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            # Proper mixed precision validation\n",
        "            if use_amp:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(imgs)\n",
        "                    # Convert to float32 for loss computation\n",
        "                    loss = criterion(outputs.float(), labels)\n",
        "            else:\n",
        "                outputs = model(imgs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            predicted = outputs.argmax(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return total_loss/total, correct/total, all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr_LmkTy1IKE",
        "outputId": "c5239f4e-3854-4c20-f5d8-0e2eafe64d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU Memory - Allocated: 0.28GB, Reserved: 0.29GB\n",
            "Starting training...\n",
            "Epoch 1 [0/36] Loss: 1.8856 Acc: 18.75% LR: 0.000032\n",
            "\n",
            "Epoch 1/40\n",
            "Train Loss: 1.4155 | Train Acc: 0.3249\n",
            "Val Loss: 1.0434 | Val Acc: 0.5587\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.5587\n",
            "Epoch 2 [0/36] Loss: 1.0171 Acc: 71.88% LR: 0.000152\n",
            "\n",
            "Epoch 2/40\n",
            "Train Loss: 1.0070 | Train Acc: 0.6017\n",
            "Val Loss: 0.8093 | Val Acc: 0.7652\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.7652\n",
            "Epoch 3 [0/36] Loss: 0.8327 Acc: 71.88% LR: 0.000429\n",
            "\n",
            "Epoch 3/40\n",
            "Train Loss: 0.7561 | Train Acc: 0.7581\n",
            "Val Loss: 0.8326 | Val Acc: 0.6842\n",
            "--------------------------------------------------\n",
            "Epoch 4 [0/36] Loss: 0.7461 Acc: 75.00% LR: 0.000698\n",
            "\n",
            "Epoch 4/40\n",
            "Train Loss: 0.6566 | Train Acc: 0.8245\n",
            "Val Loss: 0.6536 | Val Acc: 0.8259\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8259\n",
            "Epoch 5 [0/36] Loss: 0.6686 Acc: 78.12% LR: 0.000800\n",
            "\n",
            "Epoch 5/40\n",
            "Train Loss: 0.5960 | Train Acc: 0.8454\n",
            "Val Loss: 0.6775 | Val Acc: 0.8259\n",
            "--------------------------------------------------\n",
            "Epoch 6 [0/36] Loss: 1.0517 Acc: 59.38% LR: 0.000798\n",
            "\n",
            "Epoch 6/40\n",
            "Train Loss: 0.5916 | Train Acc: 0.8515\n",
            "Val Loss: 0.5977 | Val Acc: 0.8097\n",
            "--------------------------------------------------\n",
            "Epoch 7 [0/36] Loss: 0.5247 Acc: 87.50% LR: 0.000794\n",
            "\n",
            "Epoch 7/40\n",
            "Train Loss: 0.5884 | Train Acc: 0.8515\n",
            "Val Loss: 0.5928 | Val Acc: 0.8421\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8421\n",
            "Epoch 8 [0/36] Loss: 0.4945 Acc: 93.75% LR: 0.000786\n",
            "\n",
            "Epoch 8/40\n",
            "Train Loss: 0.5387 | Train Acc: 0.8742\n",
            "Val Loss: 0.6640 | Val Acc: 0.8138\n",
            "--------------------------------------------------\n",
            "Epoch 9 [0/36] Loss: 0.4987 Acc: 84.38% LR: 0.000775\n",
            "\n",
            "Epoch 9/40\n",
            "Train Loss: 0.5476 | Train Acc: 0.8751\n",
            "Val Loss: 0.6021 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 10 [0/36] Loss: 0.4707 Acc: 90.62% LR: 0.000762\n",
            "\n",
            "Epoch 10/40\n",
            "Train Loss: 0.5048 | Train Acc: 0.8961\n",
            "Val Loss: 0.6287 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 11 [0/36] Loss: 0.6266 Acc: 78.12% LR: 0.000745\n",
            "\n",
            "Epoch 11/40\n",
            "Train Loss: 0.5388 | Train Acc: 0.8681\n",
            "Val Loss: 0.5912 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 12 [0/36] Loss: 0.5582 Acc: 81.25% LR: 0.000727\n",
            "\n",
            "Epoch 12/40\n",
            "Train Loss: 0.5275 | Train Acc: 0.8812\n",
            "Val Loss: 0.6324 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 13 [0/36] Loss: 0.5532 Acc: 90.62% LR: 0.000705\n",
            "\n",
            "Epoch 13/40\n",
            "Train Loss: 0.5195 | Train Acc: 0.8786\n",
            "Val Loss: 0.5899 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 14 [0/36] Loss: 0.5757 Acc: 84.38% LR: 0.000681\n",
            "\n",
            "Epoch 14/40\n",
            "Train Loss: 0.5068 | Train Acc: 0.8926\n",
            "Val Loss: 0.6141 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 15 [0/36] Loss: 0.5656 Acc: 87.50% LR: 0.000656\n",
            "\n",
            "Epoch 15/40\n",
            "Train Loss: 0.5198 | Train Acc: 0.8769\n",
            "Val Loss: 0.5860 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Early stopping after 15 epochs\n",
            "✅ Training setup complete with EfficientNetV2-S!\n"
          ]
        }
      ],
      "source": [
        "# 10. Enhanced Training Loop with Early Stopping\n",
        "def train_model(train_loader, val_loader, epochs=40, patience=8):\n",
        "    best_acc = 0\n",
        "    no_improve = 0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accs, val_accs = [], []\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        try:\n",
        "            # Training\n",
        "            train_loss, train_acc = train_epoch(train_loader, epoch)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_acc, val_preds, val_labels = validate_epoch(val_loader)\n",
        "\n",
        "            # Save metrics\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accs.append(train_acc)\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "            print(f\"\\nEpoch {epoch}/{epochs}\")\n",
        "            print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                no_improve = 0\n",
        "\n",
        "                # Create models directory if it doesn't exist\n",
        "                models_dir = MODELS_DIR\n",
        "                models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'best_acc': best_acc,\n",
        "                    'train_losses': train_losses,\n",
        "                    'val_losses': val_losses,\n",
        "                    'train_accs': train_accs,\n",
        "                    'val_accs': val_accs,\n",
        "                    'model_config': {\n",
        "                        'num_classes': 4,\n",
        "                        'model_name': 'tf_efficientnetv2_s'  # Updated model name\n",
        "                    }\n",
        "                }, models_dir / 'best_ecg_model.pth')\n",
        "                print(f\"✅ New best model saved! Validation Accuracy: {best_acc:.4f}\")\n",
        "            else:\n",
        "                no_improve += 1\n",
        "\n",
        "            # Update scheduler if it's not OneCycleLR\n",
        "            if not isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
        "                scheduler.step()\n",
        "\n",
        "            # Early stopping\n",
        "            if no_improve >= patience:\n",
        "                print(f\"Early stopping after {epoch} epochs\")\n",
        "                break\n",
        "\n",
        "            # Memory cleanup - More frequent for smaller GPU memory\n",
        "            if epoch % 3 == 0:  # Changed from 5 to 3\n",
        "                gc.collect()\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in epoch {epoch}: {str(e)}\")\n",
        "            print(\"Continuing to next epoch...\")\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'best_acc': best_acc\n",
        "    }\n",
        "\n",
        "# Memory monitoring function for Colab\n",
        "def check_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated() / 1024**3  # Convert to GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
        "        return allocated, reserved\n",
        "    return 0, 0\n",
        "\n",
        "# Example usage:\n",
        "check_gpu_memory()  # Check initial memory usage\n",
        "results = train_model(train_loader, val_loader, epochs=40, patience=8)\n",
        "\n",
        "print(\"✅ Training setup complete with EfficientNetV2-S!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "33e05f1233f34951aae9d04a9550e0d8",
            "1e1e43b734c044d494b119dbf8db2c29",
            "4d8a77048d6b4b14be4bda8f5d52357a",
            "6446fa70ea2f4960a5fedcca206f4a80",
            "939170c5f5a94559b9b409b43ff11619",
            "edebf3fad31e41e1ace8a7168ea06582",
            "8e488f9a3f694e9c84114576293c5c65",
            "1fbf668f1bc44a41ab4909c625281486",
            "b9b61a45a36041c0ace462f0be78cf7c",
            "637e60293ba44f15a5b2c1af0b2073c4",
            "c916e0d7ca42472abfa7f53f20754ad0",
            "e5d3ebcfc36e4fcc9fd6d336905c7b68",
            "691c18d74a974823a187dd82c6942f2a",
            "6becdea84fce4e50b202393c4f127511",
            "242617d20f3e47f485335eeb80b3d65c",
            "41ab583f11604f7582539116c500d4c0",
            "32bbbed09ced4246bf41c6f3f4460c70",
            "5c7698a44af54eaf9ba87077fabcaee2",
            "485d839fde4e409b81c7036769eab8b4",
            "8527ca023b3c49a18c1d6015b9416ed5",
            "7664301a89e143aa86d981e2c66a1b89",
            "e0a28c0e5e504a71a7d7d0b99e8851ad"
          ]
        },
        "id": "uO6XUChL-5ZF",
        "outputId": "3fa7ca15-9a6c-4904-aead-1ccebbe1c66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔁 Training with model: tf_efficientnetv2_s\n",
            "Starting training...\n",
            "Epoch 1 [0/36] Loss: 1.6513 Acc: 28.12% LR: 0.000032\n",
            "\n",
            "Epoch 1/40\n",
            "Train Loss: 1.3873 | Train Acc: 0.3572\n",
            "Val Loss: 1.1001 | Val Acc: 0.5344\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.5344\n",
            "Epoch 2 [0/36] Loss: 1.0868 Acc: 62.50% LR: 0.000152\n",
            "\n",
            "Epoch 2/40\n",
            "Train Loss: 1.0102 | Train Acc: 0.5904\n",
            "Val Loss: 0.8211 | Val Acc: 0.7490\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.7490\n",
            "Epoch 3 [0/36] Loss: 0.9051 Acc: 62.50% LR: 0.000429\n",
            "\n",
            "Epoch 3/40\n",
            "Train Loss: 0.7684 | Train Acc: 0.7450\n",
            "Val Loss: 0.7146 | Val Acc: 0.7935\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.7935\n",
            "Epoch 4 [0/36] Loss: 0.6507 Acc: 81.25% LR: 0.000698\n",
            "\n",
            "Epoch 4/40\n",
            "Train Loss: 0.6840 | Train Acc: 0.8096\n",
            "Val Loss: 0.8009 | Val Acc: 0.7814\n",
            "--------------------------------------------------\n",
            "Epoch 5 [0/36] Loss: 0.6041 Acc: 84.38% LR: 0.000800\n",
            "\n",
            "Epoch 5/40\n",
            "Train Loss: 0.6623 | Train Acc: 0.8227\n",
            "Val Loss: 0.6153 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8381\n",
            "Epoch 6 [0/36] Loss: 0.6536 Acc: 78.12% LR: 0.000798\n",
            "\n",
            "Epoch 6/40\n",
            "Train Loss: 0.6090 | Train Acc: 0.8306\n",
            "Val Loss: 0.7080 | Val Acc: 0.7854\n",
            "--------------------------------------------------\n",
            "Epoch 7 [0/36] Loss: 0.5693 Acc: 90.62% LR: 0.000794\n",
            "\n",
            "Epoch 7/40\n",
            "Train Loss: 0.5902 | Train Acc: 0.8524\n",
            "Val Loss: 0.6323 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 8 [0/36] Loss: 0.5755 Acc: 84.38% LR: 0.000786\n",
            "\n",
            "Epoch 8/40\n",
            "Train Loss: 0.5729 | Train Acc: 0.8568\n",
            "Val Loss: 0.6067 | Val Acc: 0.8259\n",
            "--------------------------------------------------\n",
            "Epoch 9 [0/36] Loss: 0.5896 Acc: 90.62% LR: 0.000775\n",
            "\n",
            "Epoch 9/40\n",
            "Train Loss: 0.5295 | Train Acc: 0.8742\n",
            "Val Loss: 0.5757 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 10 [0/36] Loss: 0.5572 Acc: 84.38% LR: 0.000762\n",
            "\n",
            "Epoch 10/40\n",
            "Train Loss: 0.5643 | Train Acc: 0.8472\n",
            "Val Loss: 0.6066 | Val Acc: 0.8421\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8421\n",
            "Epoch 11 [0/36] Loss: 0.4811 Acc: 90.62% LR: 0.000745\n",
            "\n",
            "Epoch 11/40\n",
            "Train Loss: 0.5583 | Train Acc: 0.8603\n",
            "Val Loss: 0.5897 | Val Acc: 0.8259\n",
            "--------------------------------------------------\n",
            "Epoch 12 [0/36] Loss: 0.5220 Acc: 93.75% LR: 0.000727\n",
            "\n",
            "Epoch 12/40\n",
            "Train Loss: 0.5572 | Train Acc: 0.8620\n",
            "Val Loss: 0.5769 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 13 [0/36] Loss: 0.5319 Acc: 87.50% LR: 0.000705\n",
            "\n",
            "Epoch 13/40\n",
            "Train Loss: 0.5317 | Train Acc: 0.8751\n",
            "Val Loss: 0.5826 | Val Acc: 0.8421\n",
            "--------------------------------------------------\n",
            "Epoch 14 [0/36] Loss: 0.6093 Acc: 84.38% LR: 0.000681\n",
            "\n",
            "Epoch 14/40\n",
            "Train Loss: 0.5120 | Train Acc: 0.8812\n",
            "Val Loss: 0.6404 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 15 [0/36] Loss: 0.4447 Acc: 93.75% LR: 0.000656\n",
            "\n",
            "Epoch 15/40\n",
            "Train Loss: 0.5250 | Train Acc: 0.8760\n",
            "Val Loss: 0.5950 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 16 [0/36] Loss: 0.4455 Acc: 93.75% LR: 0.000628\n",
            "\n",
            "Epoch 16/40\n",
            "Train Loss: 0.4911 | Train Acc: 0.9057\n",
            "Val Loss: 0.5890 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 17 [0/36] Loss: 0.5020 Acc: 87.50% LR: 0.000598\n",
            "\n",
            "Epoch 17/40\n",
            "Train Loss: 0.5042 | Train Acc: 0.8865\n",
            "Val Loss: 0.5974 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 18 [0/36] Loss: 0.5093 Acc: 87.50% LR: 0.000567\n",
            "\n",
            "Epoch 18/40\n",
            "Train Loss: 0.5374 | Train Acc: 0.8699\n",
            "Val Loss: 0.5839 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Early stopping after 18 epochs\n",
            "✅ Finished tf_efficientnetv2_s | Best Val Accuracy: 0.8421\n",
            "------------------------------------------------------------\n",
            "\n",
            "🔁 Training with model: tf_efficientnetv2_m\n",
            "Starting training...\n",
            "Epoch 1 [0/36] Loss: 1.5757 Acc: 25.00% LR: 0.000032\n",
            "\n",
            "Epoch 1/40\n",
            "Train Loss: 1.3636 | Train Acc: 0.3869\n",
            "Val Loss: 1.1138 | Val Acc: 0.4737\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.4737\n",
            "Epoch 2 [0/36] Loss: 1.3121 Acc: 34.38% LR: 0.000152\n",
            "\n",
            "Epoch 2/40\n",
            "Train Loss: 0.9932 | Train Acc: 0.6114\n",
            "Val Loss: 0.9087 | Val Acc: 0.6761\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.6761\n",
            "Epoch 3 [0/36] Loss: 0.8899 Acc: 62.50% LR: 0.000429\n",
            "\n",
            "Epoch 3/40\n",
            "Train Loss: 0.8013 | Train Acc: 0.7275\n",
            "Val Loss: 0.7215 | Val Acc: 0.7854\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.7854\n",
            "Epoch 4 [0/36] Loss: 0.8063 Acc: 71.88% LR: 0.000698\n",
            "\n",
            "Epoch 4/40\n",
            "Train Loss: 0.6779 | Train Acc: 0.8061\n",
            "Val Loss: 0.6570 | Val Acc: 0.7733\n",
            "--------------------------------------------------\n",
            "Epoch 5 [0/36] Loss: 0.5479 Acc: 84.38% LR: 0.000800\n",
            "\n",
            "Epoch 5/40\n",
            "Train Loss: 0.6335 | Train Acc: 0.8314\n",
            "Val Loss: 0.6709 | Val Acc: 0.8178\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8178\n",
            "Epoch 6 [0/36] Loss: 0.5990 Acc: 81.25% LR: 0.000798\n",
            "\n",
            "Epoch 6/40\n",
            "Train Loss: 0.5981 | Train Acc: 0.8437\n",
            "Val Loss: 0.6116 | Val Acc: 0.8259\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8259\n",
            "Epoch 7 [0/36] Loss: 0.5361 Acc: 90.62% LR: 0.000794\n",
            "\n",
            "Epoch 7/40\n",
            "Train Loss: 0.5638 | Train Acc: 0.8699\n",
            "Val Loss: 0.6073 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8340\n",
            "Epoch 8 [0/36] Loss: 0.4588 Acc: 96.88% LR: 0.000786\n",
            "\n",
            "Epoch 8/40\n",
            "Train Loss: 0.5331 | Train Acc: 0.8830\n",
            "Val Loss: 0.6096 | Val Acc: 0.8259\n",
            "--------------------------------------------------\n",
            "Epoch 9 [0/36] Loss: 0.4164 Acc: 96.88% LR: 0.000775\n",
            "\n",
            "Epoch 9/40\n",
            "Train Loss: 0.5288 | Train Acc: 0.8856\n",
            "Val Loss: 0.6051 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 10 [0/36] Loss: 0.5397 Acc: 81.25% LR: 0.000762\n",
            "\n",
            "Epoch 10/40\n",
            "Train Loss: 0.5235 | Train Acc: 0.8891\n",
            "Val Loss: 0.5720 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8381\n",
            "Epoch 11 [0/36] Loss: 0.5139 Acc: 90.62% LR: 0.000745\n",
            "\n",
            "Epoch 11/40\n",
            "Train Loss: 0.5345 | Train Acc: 0.8690\n",
            "Val Loss: 0.5915 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 12 [0/36] Loss: 0.5893 Acc: 84.38% LR: 0.000727\n",
            "\n",
            "Epoch 12/40\n",
            "Train Loss: 0.5075 | Train Acc: 0.8873\n",
            "Val Loss: 0.5859 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 13 [0/36] Loss: 0.6002 Acc: 81.25% LR: 0.000705\n",
            "\n",
            "Epoch 13/40\n",
            "Train Loss: 0.5173 | Train Acc: 0.8908\n",
            "Val Loss: 0.6398 | Val Acc: 0.8219\n",
            "--------------------------------------------------\n",
            "Epoch 14 [0/36] Loss: 0.6547 Acc: 78.12% LR: 0.000681\n",
            "\n",
            "Epoch 14/40\n",
            "Train Loss: 0.5240 | Train Acc: 0.8847\n",
            "Val Loss: 0.5992 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 15 [0/36] Loss: 0.5302 Acc: 84.38% LR: 0.000656\n",
            "\n",
            "Epoch 15/40\n",
            "Train Loss: 0.5091 | Train Acc: 0.8812\n",
            "Val Loss: 0.5710 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 16 [0/36] Loss: 0.4492 Acc: 93.75% LR: 0.000628\n",
            "\n",
            "Epoch 16/40\n",
            "Train Loss: 0.5055 | Train Acc: 0.8821\n",
            "Val Loss: 0.5922 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 17 [0/36] Loss: 0.4719 Acc: 90.62% LR: 0.000598\n",
            "\n",
            "Epoch 17/40\n",
            "Train Loss: 0.4883 | Train Acc: 0.8926\n",
            "Val Loss: 0.6081 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 18 [0/36] Loss: 0.4090 Acc: 96.88% LR: 0.000567\n",
            "\n",
            "Epoch 18/40\n",
            "Train Loss: 0.5199 | Train Acc: 0.8769\n",
            "Val Loss: 0.5851 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Early stopping after 18 epochs\n",
            "✅ Finished tf_efficientnetv2_m | Best Val Accuracy: 0.8381\n",
            "------------------------------------------------------------\n",
            "\n",
            "🔁 Training with model: convnext_tiny\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33e05f1233f34951aae9d04a9550e0d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/114M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 1 [0/36] Loss: 2.5545 Acc: 28.12% LR: 0.000032\n",
            "\n",
            "Epoch 1/40\n",
            "Train Loss: 1.8067 | Train Acc: 0.2376\n",
            "Val Loss: 1.4515 | Val Acc: 0.2065\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.2065\n",
            "Epoch 2 [0/36] Loss: 1.3326 Acc: 28.12% LR: 0.000152\n",
            "\n",
            "Epoch 2/40\n",
            "Train Loss: 1.3910 | Train Acc: 0.2786\n",
            "Val Loss: 1.4830 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Epoch 3 [0/36] Loss: 1.3722 Acc: 21.88% LR: 0.000429\n",
            "\n",
            "Epoch 3/40\n",
            "Train Loss: 1.3779 | Train Acc: 0.2559\n",
            "Val Loss: 1.4671 | Val Acc: 0.2065\n",
            "--------------------------------------------------\n",
            "Epoch 4 [0/36] Loss: 1.4198 Acc: 25.00% LR: 0.000698\n",
            "\n",
            "Epoch 4/40\n",
            "Train Loss: 1.3584 | Train Acc: 0.2699\n",
            "Val Loss: 1.4127 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Epoch 5 [0/36] Loss: 1.3919 Acc: 15.62% LR: 0.000800\n",
            "\n",
            "Epoch 5/40\n",
            "Train Loss: 1.3871 | Train Acc: 0.2192\n",
            "Val Loss: 1.4313 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Epoch 6 [0/36] Loss: 1.2470 Acc: 34.38% LR: 0.000798\n",
            "\n",
            "Epoch 6/40\n",
            "Train Loss: 1.3697 | Train Acc: 0.2323\n",
            "Val Loss: 1.4277 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Epoch 7 [0/36] Loss: 1.3057 Acc: 37.50% LR: 0.000794\n",
            "\n",
            "Epoch 7/40\n",
            "Train Loss: 1.3540 | Train Acc: 0.2463\n",
            "Val Loss: 1.4470 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Epoch 8 [0/36] Loss: 1.3704 Acc: 18.75% LR: 0.000786\n",
            "\n",
            "Epoch 8/40\n",
            "Train Loss: 1.3620 | Train Acc: 0.2376\n",
            "Val Loss: 1.4207 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Epoch 9 [0/36] Loss: 1.3379 Acc: 25.00% LR: 0.000775\n",
            "\n",
            "Epoch 9/40\n",
            "Train Loss: 1.3555 | Train Acc: 0.2419\n",
            "Val Loss: 1.4240 | Val Acc: 0.1579\n",
            "--------------------------------------------------\n",
            "Early stopping after 9 epochs\n",
            "✅ Finished convnext_tiny | Best Val Accuracy: 0.2065\n",
            "------------------------------------------------------------\n",
            "\n",
            "🔁 Training with model: resnext50_32x4d\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5d3ebcfc36e4fcc9fd6d336905c7b68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/100M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 1 [0/36] Loss: 1.3745 Acc: 28.12% LR: 0.000032\n",
            "\n",
            "Epoch 1/40\n",
            "Train Loss: 1.3614 | Train Acc: 0.2760\n",
            "Val Loss: 1.3950 | Val Acc: 0.1741\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.1741\n",
            "Epoch 2 [0/36] Loss: 1.3658 Acc: 28.12% LR: 0.000152\n",
            "\n",
            "Epoch 2/40\n",
            "Train Loss: 1.2143 | Train Acc: 0.3668\n",
            "Val Loss: 1.0948 | Val Acc: 0.4899\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.4899\n",
            "Epoch 3 [0/36] Loss: 1.1255 Acc: 50.00% LR: 0.000429\n",
            "\n",
            "Epoch 3/40\n",
            "Train Loss: 0.8874 | Train Acc: 0.6568\n",
            "Val Loss: 0.7849 | Val Acc: 0.7287\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.7287\n",
            "Epoch 4 [0/36] Loss: 0.9752 Acc: 65.62% LR: 0.000698\n",
            "\n",
            "Epoch 4/40\n",
            "Train Loss: 0.7141 | Train Acc: 0.7799\n",
            "Val Loss: 0.6964 | Val Acc: 0.7976\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.7976\n",
            "Epoch 5 [0/36] Loss: 0.5534 Acc: 87.50% LR: 0.000800\n",
            "\n",
            "Epoch 5/40\n",
            "Train Loss: 0.6405 | Train Acc: 0.8166\n",
            "Val Loss: 0.6420 | Val Acc: 0.8178\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8178\n",
            "Epoch 6 [0/36] Loss: 0.5393 Acc: 90.62% LR: 0.000798\n",
            "\n",
            "Epoch 6/40\n",
            "Train Loss: 0.5893 | Train Acc: 0.8507\n",
            "Val Loss: 0.6663 | Val Acc: 0.8057\n",
            "--------------------------------------------------\n",
            "Epoch 7 [0/36] Loss: 0.5162 Acc: 90.62% LR: 0.000794\n",
            "\n",
            "Epoch 7/40\n",
            "Train Loss: 0.5337 | Train Acc: 0.8803\n",
            "Val Loss: 0.6162 | Val Acc: 0.8178\n",
            "--------------------------------------------------\n",
            "Epoch 8 [0/36] Loss: 0.6292 Acc: 81.25% LR: 0.000786\n",
            "\n",
            "Epoch 8/40\n",
            "Train Loss: 0.5471 | Train Acc: 0.8716\n",
            "Val Loss: 0.6439 | Val Acc: 0.8057\n",
            "--------------------------------------------------\n",
            "Epoch 9 [0/36] Loss: 0.4743 Acc: 87.50% LR: 0.000775\n",
            "\n",
            "Epoch 9/40\n",
            "Train Loss: 0.5368 | Train Acc: 0.8734\n",
            "Val Loss: 0.5977 | Val Acc: 0.8097\n",
            "--------------------------------------------------\n",
            "Epoch 10 [0/36] Loss: 0.5598 Acc: 84.38% LR: 0.000762\n",
            "\n",
            "Epoch 10/40\n",
            "Train Loss: 0.5442 | Train Acc: 0.8760\n",
            "Val Loss: 0.6081 | Val Acc: 0.8138\n",
            "--------------------------------------------------\n",
            "Epoch 11 [0/36] Loss: 0.5131 Acc: 84.38% LR: 0.000745\n",
            "\n",
            "Epoch 11/40\n",
            "Train Loss: 0.5465 | Train Acc: 0.8672\n",
            "Val Loss: 0.5869 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8340\n",
            "Epoch 12 [0/36] Loss: 0.4772 Acc: 93.75% LR: 0.000727\n",
            "\n",
            "Epoch 12/40\n",
            "Train Loss: 0.5274 | Train Acc: 0.8821\n",
            "Val Loss: 0.6272 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 13 [0/36] Loss: 0.4702 Acc: 90.62% LR: 0.000705\n",
            "\n",
            "Epoch 13/40\n",
            "Train Loss: 0.5242 | Train Acc: 0.8847\n",
            "Val Loss: 0.6154 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 14 [0/36] Loss: 0.5460 Acc: 87.50% LR: 0.000681\n",
            "\n",
            "Epoch 14/40\n",
            "Train Loss: 0.5026 | Train Acc: 0.8978\n",
            "Val Loss: 0.6149 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 15 [0/36] Loss: 0.3976 Acc: 96.88% LR: 0.000656\n",
            "\n",
            "Epoch 15/40\n",
            "Train Loss: 0.5290 | Train Acc: 0.8716\n",
            "Val Loss: 0.6103 | Val Acc: 0.8219\n",
            "--------------------------------------------------\n",
            "Epoch 16 [0/36] Loss: 0.4651 Acc: 93.75% LR: 0.000628\n",
            "\n",
            "Epoch 16/40\n",
            "Train Loss: 0.5158 | Train Acc: 0.8856\n",
            "Val Loss: 0.6058 | Val Acc: 0.8138\n",
            "--------------------------------------------------\n",
            "Epoch 17 [0/36] Loss: 0.5445 Acc: 84.38% LR: 0.000598\n",
            "\n",
            "Epoch 17/40\n",
            "Train Loss: 0.5154 | Train Acc: 0.8856\n",
            "Val Loss: 0.5902 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 18 [0/36] Loss: 0.5211 Acc: 90.62% LR: 0.000567\n",
            "\n",
            "Epoch 18/40\n",
            "Train Loss: 0.5203 | Train Acc: 0.8795\n",
            "Val Loss: 0.6998 | Val Acc: 0.8138\n",
            "--------------------------------------------------\n",
            "Epoch 19 [0/36] Loss: 0.4251 Acc: 93.75% LR: 0.000535\n",
            "\n",
            "Epoch 19/40\n",
            "Train Loss: 0.5201 | Train Acc: 0.8803\n",
            "Val Loss: 0.5769 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "✅ New best model saved! Validation Accuracy: 0.8381\n",
            "Epoch 20 [0/36] Loss: 0.4879 Acc: 90.62% LR: 0.000502\n",
            "\n",
            "Epoch 20/40\n",
            "Train Loss: 0.4972 | Train Acc: 0.8943\n",
            "Val Loss: 0.5839 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 21 [0/36] Loss: 0.4378 Acc: 90.62% LR: 0.000468\n",
            "\n",
            "Epoch 21/40\n",
            "Train Loss: 0.4989 | Train Acc: 0.8961\n",
            "Val Loss: 0.5794 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 22 [0/36] Loss: 0.4612 Acc: 93.75% LR: 0.000433\n",
            "\n",
            "Epoch 22/40\n",
            "Train Loss: 0.5157 | Train Acc: 0.8707\n",
            "Val Loss: 0.5831 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Epoch 23 [0/36] Loss: 0.5409 Acc: 87.50% LR: 0.000398\n",
            "\n",
            "Epoch 23/40\n",
            "Train Loss: 0.5050 | Train Acc: 0.8856\n",
            "Val Loss: 0.5739 | Val Acc: 0.8340\n",
            "--------------------------------------------------\n",
            "Epoch 24 [0/36] Loss: 0.4427 Acc: 93.75% LR: 0.000363\n",
            "\n",
            "Epoch 24/40\n",
            "Train Loss: 0.4637 | Train Acc: 0.9179\n",
            "Val Loss: 0.5919 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 25 [0/36] Loss: 0.3982 Acc: 96.88% LR: 0.000329\n",
            "\n",
            "Epoch 25/40\n",
            "Train Loss: 0.5005 | Train Acc: 0.8882\n",
            "Val Loss: 0.5617 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 26 [0/36] Loss: 0.4652 Acc: 93.75% LR: 0.000295\n",
            "\n",
            "Epoch 26/40\n",
            "Train Loss: 0.5005 | Train Acc: 0.8812\n",
            "Val Loss: 0.5840 | Val Acc: 0.8381\n",
            "--------------------------------------------------\n",
            "Epoch 27 [0/36] Loss: 0.6546 Acc: 84.38% LR: 0.000261\n",
            "\n",
            "Epoch 27/40\n",
            "Train Loss: 0.4939 | Train Acc: 0.8943\n",
            "Val Loss: 0.5903 | Val Acc: 0.8300\n",
            "--------------------------------------------------\n",
            "Early stopping after 27 epochs\n",
            "✅ Finished resnext50_32x4d | Best Val Accuracy: 0.8381\n",
            "------------------------------------------------------------\n",
            "\n",
            "🔁 Training with model: coatnet_0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unknown model (coatnet_0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-2017013230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Reinitialize model and move to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mECGClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Redefine optimizer and scheduler for each model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-26-2577111882.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, num_classes, dropout)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_efficientnetv2_s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/_factory.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, cache_dir, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown model (%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mcreate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_entrypoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unknown model (coatnet_0)"
          ]
        }
      ],
      "source": [
        "# 11. Try Multiple Models for Comparison - EfficientNet-B0, EfficientNetV2-S, ConvNeXt, ResNeXt\n",
        "model_names = [\n",
        "    'efficientnet_b0',\n",
        "    'tf_efficientnetv2_s',\n",
        "    'resnext50_32x4d',\n",
        "]\n",
        "\n",
        "comparison_results = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f\"\\n🔁 Training with model: {model_name}\")\n",
        "\n",
        "    # Reinitialize model and move to device\n",
        "    model = ECGClassifier(model_name=model_name, num_classes=4).to(device)\n",
        "\n",
        "    # Redefine optimizer and scheduler for each model\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=8e-4,\n",
        "        weight_decay=0.01,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=8e-4,\n",
        "        epochs=40,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    # Redefine scaler for mixed precision\n",
        "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
        "\n",
        "    # Train the model\n",
        "    result = train_model(train_loader, val_loader, epochs=40, patience=8)\n",
        "\n",
        "    # Store best val acc for comparison\n",
        "    comparison_results[model_name] = result['best_acc']\n",
        "\n",
        "    print(f\"✅ Finished {model_name} | Best Val Accuracy: {result['best_acc']:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# Summary of all model performances\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "for name, acc in comparison_results.items():\n",
        "    print(f\"{name}: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuHM_iIL1QH5"
      },
      "outputs": [],
      "source": [
        "# 11. Training Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the training metrics from results\n",
        "train_losses = results['train_losses']\n",
        "val_losses = results['val_losses']\n",
        "train_accs = results['train_accs']\n",
        "val_accs = results['val_accs']\n",
        "\n",
        "# Create visualization\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss curves\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "ax1.plot(epochs, train_losses, label='Training Loss', color='blue', linewidth=2)\n",
        "ax1.plot(epochs, val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy curves\n",
        "ax2.plot(epochs, train_accs, label='Training Accuracy', color='blue', linewidth=2)\n",
        "ax2.plot(epochs, val_accs, label='Validation Accuracy', color='red', linewidth=2)\n",
        "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate schedule (approximate for OneCycleLR)\n",
        "# Since we can't track exact LR, we'll create an approximation\n",
        "max_lr = 8e-4\n",
        "total_epochs = len(train_losses)\n",
        "# OneCycleLR pattern: rise to max, then decay\n",
        "lr_schedule = []\n",
        "for i in range(total_epochs):\n",
        "    if i < total_epochs * 0.1:  # First 10% - warmup\n",
        "        lr = max_lr * (i / (total_epochs * 0.1))\n",
        "    else:  # Remaining 90% - cosine decay\n",
        "        progress = (i - total_epochs * 0.1) / (total_epochs * 0.9)\n",
        "        lr = max_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "    lr_schedule.append(lr)\n",
        "\n",
        "ax3.plot(epochs, lr_schedule, color='green', linewidth=2)\n",
        "ax3.set_title('Learning Rate Schedule (Approximate)', fontsize=14, fontweight='bold')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Learning Rate')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Training summary\n",
        "ax4.text(0.1, 0.8, f'Best Validation Accuracy: {results[\"best_acc\"]:.4f}',\n",
        "         fontsize=12, transform=ax4.transAxes, fontweight='bold')\n",
        "ax4.text(0.1, 0.7, f'Final Training Accuracy: {train_accs[-1]:.4f}',\n",
        "         fontsize=12, transform=ax4.transAxes)\n",
        "ax4.text(0.1, 0.6, f'Final Validation Accuracy: {val_accs[-1]:.4f}',\n",
        "         fontsize=12, transform=ax4.transAxes)\n",
        "ax4.text(0.1, 0.5, f'Total Epochs: {len(train_losses)}',\n",
        "         fontsize=12, transform=ax4.transAxes)\n",
        "ax4.text(0.1, 0.4, f'Early Stopping: {\"Yes\" if len(train_losses) < 40 else \"No\"}',\n",
        "         fontsize=12, transform=ax4.transAxes)\n",
        "ax4.set_title('Training Summary', fontsize=14, fontweight='bold')\n",
        "ax4.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG0ZAj-z1QqB"
      },
      "outputs": [],
      "source": [
        "# 12. Comprehensive Model Evaluation\n",
        "print(\"Loading best model...\")\n",
        "\n",
        "# Model path - use the correct directory structure\n",
        "models_dir = MODELS_DIR\n",
        "model_path = models_dir / 'best_ecg_model.pth'\n",
        "\n",
        "# Check if model file exists\n",
        "if model_path.exists():\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"✅ Model loaded successfully!\")\n",
        "    print(f\"Best validation accuracy: {checkpoint['best_acc']:.4f}\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\nEvaluating model on validation set...\")\n",
        "    val_loss, val_acc, y_pred, y_true = validate_epoch(val_loader)\n",
        "\n",
        "    # Classification report\n",
        "    label_names = ['Abnormal HB', 'MI', 'Normal', 'PMI']\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=label_names, yticklabels=label_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title('Confusion Matrix - ECG Classification', fontsize=16, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Additional metrics\n",
        "    from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"DETAILED METRICS BY CLASS\")\n",
        "    print(\"=\"*60)\n",
        "    for i, label in enumerate(label_names):\n",
        "        print(f\"{label:12} | Precision: {precision[i]:.4f} | Recall: {recall[i]:.4f} | F1: {f1[i]:.4f} | Support: {support[i]}\")\n",
        "\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Macro Average F1: {np.mean(f1):.4f}\")\n",
        "    print(f\"Weighted Average F1: {np.average(f1, weights=support):.4f}\")\n",
        "\n",
        "else:\n",
        "    print(f\"❌ Model file not found at: {model_path}\")\n",
        "    print(\"Available files in models directory:\")\n",
        "    if models_dir.exists():\n",
        "        for file in models_dir.iterdir():\n",
        "            print(f\"  - {file.name}\")\n",
        "    else:\n",
        "        print(\"  Models directory doesn't exist!\")\n",
        "\n",
        "    print(\"\\nUsing current model state for evaluation...\")\n",
        "    val_loss, val_acc, y_pred, y_true = validate_epoch(val_loader)\n",
        "\n",
        "    # Classification report with current model\n",
        "    label_names = ['Abnormal HB', 'MI', 'Normal', 'PMI']\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CLASSIFICATION REPORT (Current Model State)\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names, digits=4))\n",
        "\n",
        "print(\"\\n🎯 Model evaluation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9sd3RQS4013"
      },
      "outputs": [],
      "source": [
        "# 13. Clinical Report Generation\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "class_map = {\n",
        "    0: (\"Abnormal Heartbeat\", \"Possible arrhythmia or irregular rhythm. Recommend further cardiological evaluation.\"),\n",
        "    1: (\"Myocardial Infarction\", \"ECG pattern consistent with myocardial infarction. Urgent clinical attention advised.\"),\n",
        "    2: (\"Normal Sinus Rhythm\", \"Normal ECG. No abnormalities detected.\"),\n",
        "    3: (\"History of Myocardial Infarction\", \"Signs of previous infarction. Regular follow-up recommended.\")\n",
        "}\n",
        "\n",
        "def generate_text_report(test_id, prediction_idx, save_dir):\n",
        "    date_str = datetime.now().strftime(\"%Y‑%m‑%d\")\n",
        "    diagnosis, interpretation = class_map[prediction_idx]\n",
        "\n",
        "    content = f\"\"\"Test ID: {test_id}\n",
        "Date: {date_str}\n",
        "\n",
        "Automated Diagnosis\n",
        "-------------------\n",
        "{diagnosis}\n",
        "\n",
        "Clinical Interpretation\n",
        "-----------------------\n",
        "{interpretation}\n",
        "\"\"\"\n",
        "\n",
        "    report_path = save_dir / f\"{test_id}.txt\"\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(content)\n",
        "    return diagnosis\n",
        "\n",
        "\n",
        "REPORTS_DIR.mkdir(exist_ok=True)\n",
        "summary_data = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Use your test loader, or raw paths like test_paths\n",
        "test_ds = ECGImageDataset(TEST_DIR, transform=get_validation_transforms())\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
        "\n",
        "print(\"Generating reports...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for img, _, path_str in tqdm(test_loader):\n",
        "      img = img.to(device)\n",
        "      path = Path(path_str[0])\n",
        "      test_id = path.stem\n",
        "\n",
        "      start_time = time.time()\n",
        "\n",
        "      # Prediction\n",
        "      output = model(img)\n",
        "      pred_idx = output.argmax(1).item()\n",
        "\n",
        "      end_time = time.time()\n",
        "      exec_time = round(end_time - start_time, 4)  # seconds\n",
        "\n",
        "      # Save report\n",
        "      diagnosis = generate_text_report(test_id, pred_idx, REPORTS_DIR)\n",
        "\n",
        "      # Append to summary\n",
        "      summary_data.append({\n",
        "          \"Test ID\": test_id,\n",
        "          \"Prediction Class\": pred_idx,\n",
        "          \"Diagnosis\": class_map[pred_idx][0],\n",
        "          \"Execution Time (s)\": exec_time\n",
        "      })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df.to_csv(REPORTS_DIR / \"ecg_summary.csv\", index=False)\n",
        "print(\"✅ Summary CSV saved at:\", REPORTS_DIR / \"ecg_summary.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMhYfqb07B9v"
      },
      "outputs": [],
      "source": [
        "# 14. ROC Curve Analysis with Enhanced Visualization\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc_curves(model, dataloader, n_classes=4):\n",
        "    model.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels, _ in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Binarize the labels\n",
        "    y_true = label_binarize(all_labels, classes=list(range(n_classes)))\n",
        "    y_score = np.array(all_probs)\n",
        "\n",
        "    fpr, tpr, roc_auc = {}, {}, {}\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['red', 'green', 'blue', 'purple']\n",
        "    labels = ['Abnormal Heartbeat', 'Myocardial Infarction', 'Normal', 'PMI']\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        plt.plot(fpr[i], tpr[i], color=colors[i],\n",
        "                 label=f'{labels[i]} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-Class ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example\n",
        "plot_roc_curves(model, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGzAXJwf7u4H"
      },
      "outputs": [],
      "source": [
        "# 15. Grad-CAM Visualization Enhancement\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as T\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "def batch_generate_gradcams(model, dataset, output_dir, method='GradCAM'):\n",
        "    model.eval()\n",
        "\n",
        "    # Choose the correct layer for EfficientNetV2-S\n",
        "    target_layer = model.backbone.blocks[-1]\n",
        "\n",
        "    cam_algorithm = eval(method)\n",
        "    cam = cam_algorithm(model=model, target_layers=[target_layer])\n",
        "\n",
        "    print(f\"Generating Grad-CAMs for {len(dataset)} samples...\")\n",
        "\n",
        "    for idx in tqdm(range(len(dataset))):\n",
        "        try:\n",
        "            image_tensor, label, path = dataset[idx]\n",
        "\n",
        "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "            # Run CAM\n",
        "            grayscale_cam = cam(input_tensor=input_tensor,\n",
        "                                targets=[ClassifierOutputTarget(label)])[0]\n",
        "\n",
        "            # Convert image for overlay\n",
        "            img_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "\n",
        "            cam_image = show_cam_on_image(img_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "            # Save CAM image\n",
        "            file_name = Path(path).stem + f'_class{label}.png'\n",
        "            save_path = output_dir / file_name\n",
        "            Image.fromarray(cam_image).save(save_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed for index {idx} ({path}): {e}\")\n",
        "\n",
        "    print(\"✅ Grad-CAM generation complete.\")\n",
        "\n",
        "# === Usage ===\n",
        "gradcam_dir = BASE_DIR / 'gradcam_outputs'\n",
        "gradcam_dir.mkdir(exist_ok=True)\n",
        "batch_generate_gradcams(model, test_ds, gradcam_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e1e43b734c044d494b119dbf8db2c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edebf3fad31e41e1ace8a7168ea06582",
            "placeholder": "​",
            "style": "IPY_MODEL_8e488f9a3f694e9c84114576293c5c65",
            "value": "model.safetensors: 100%"
          }
        },
        "1fbf668f1bc44a41ab4909c625281486": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "242617d20f3e47f485335eeb80b3d65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7664301a89e143aa86d981e2c66a1b89",
            "placeholder": "​",
            "style": "IPY_MODEL_e0a28c0e5e504a71a7d7d0b99e8851ad",
            "value": " 100M/100M [00:00&lt;00:00, 144MB/s]"
          }
        },
        "32bbbed09ced4246bf41c6f3f4460c70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33e05f1233f34951aae9d04a9550e0d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e1e43b734c044d494b119dbf8db2c29",
              "IPY_MODEL_4d8a77048d6b4b14be4bda8f5d52357a",
              "IPY_MODEL_6446fa70ea2f4960a5fedcca206f4a80"
            ],
            "layout": "IPY_MODEL_939170c5f5a94559b9b409b43ff11619"
          }
        },
        "41ab583f11604f7582539116c500d4c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485d839fde4e409b81c7036769eab8b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8a77048d6b4b14be4bda8f5d52357a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbf668f1bc44a41ab4909c625281486",
            "max": 114374272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9b61a45a36041c0ace462f0be78cf7c",
            "value": 114374272
          }
        },
        "5c7698a44af54eaf9ba87077fabcaee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "637e60293ba44f15a5b2c1af0b2073c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6446fa70ea2f4960a5fedcca206f4a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637e60293ba44f15a5b2c1af0b2073c4",
            "placeholder": "​",
            "style": "IPY_MODEL_c916e0d7ca42472abfa7f53f20754ad0",
            "value": " 114M/114M [00:00&lt;00:00, 154MB/s]"
          }
        },
        "691c18d74a974823a187dd82c6942f2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32bbbed09ced4246bf41c6f3f4460c70",
            "placeholder": "​",
            "style": "IPY_MODEL_5c7698a44af54eaf9ba87077fabcaee2",
            "value": "model.safetensors: 100%"
          }
        },
        "6becdea84fce4e50b202393c4f127511": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_485d839fde4e409b81c7036769eab8b4",
            "max": 100417784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8527ca023b3c49a18c1d6015b9416ed5",
            "value": 100417784
          }
        },
        "7664301a89e143aa86d981e2c66a1b89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8527ca023b3c49a18c1d6015b9416ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e488f9a3f694e9c84114576293c5c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "939170c5f5a94559b9b409b43ff11619": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b61a45a36041c0ace462f0be78cf7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c916e0d7ca42472abfa7f53f20754ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0a28c0e5e504a71a7d7d0b99e8851ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5d3ebcfc36e4fcc9fd6d336905c7b68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_691c18d74a974823a187dd82c6942f2a",
              "IPY_MODEL_6becdea84fce4e50b202393c4f127511",
              "IPY_MODEL_242617d20f3e47f485335eeb80b3d65c"
            ],
            "layout": "IPY_MODEL_41ab583f11604f7582539116c500d4c0"
          }
        },
        "edebf3fad31e41e1ace8a7168ea06582": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}